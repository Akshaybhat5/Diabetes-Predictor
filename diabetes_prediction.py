# -*- coding: utf-8 -*-
"""Diabetes_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VOfY7Tkvu6qVaFmZcLuUx9JC_es_IFf5

**DIABETES PREDICTION**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = pd.read_csv('diabetes.csv')

data.head()

data.info()

data.describe()

data.head(10)

# let's draw a heat map to understand the correlation between the variables
plt.figure(figsize=(15,15))
correlation = data.corr()
sns.heatmap(correlation, annot=True)
plt.title('HEAT-MAP', fontweight='bold')
plt.tight_layout()

data.isnull().sum()

# Luckily we don't have any data missing

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data['Pregnancies'])
plt.tight_layout()

# let's remove outliers by quantile method
q = data['Pregnancies'].quantile(0.99)
data_1 = data[data['Pregnancies']<q]
data_1.describe()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_1['Pregnancies'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_1['Glucose'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data['BloodPressure'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data['SkinThickness'])
plt.tight_layout()

# let's remove some of the outliers in skinthickness
q = data_1['SkinThickness'].quantile(0.99)
data_2 = data_1[data_1['SkinThickness']<q]
data_2.describe()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_2['SkinThickness'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_2['Insulin'])
plt.tight_layout()

# let's remove some of the outliers in insulin
q = data_2['Insulin'].quantile(0.99)
data_3 = data_2[data_2['Insulin']<q]
data_3.describe()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_3['Insulin'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_3['BMI'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_3['DiabetesPedigreeFunction'])
plt.tight_layout()

# let's remove some outliers
q = data_3['DiabetesPedigreeFunction'].quantile(0.99)
data_4 = data_3[data_3['DiabetesPedigreeFunction']<q]
data_4.describe()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_4['DiabetesPedigreeFunction'])
plt.tight_layout()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_4['Age'])
plt.tight_layout()

# let's remove some outliers
q = data_4['Age'].quantile(0.99)
data_5 = data_4[data_4['Age']<q]
data_5.describe()

# let's check if there is any outliers 

plt.figure(figsize=(8,8))
sns.distplot(data_5['Age'])
plt.tight_layout()

data_preprocessed = data_5

data_preprocessed.head()

data_preprocessed.describe()

sns.set_theme(style='darkgrid')
data_preprocessed.hist(bins=30, color='red', figsize=(10,10))
plt.tight_layout()

# let's calculate the number of patients having diabetes 
no_diabetes = data[data['Outcome']==0]
with_diabetes = data[data['Outcome']==1]

print(f'The people having diabetes : {len(with_diabetes)}')
print(f'The people who are safe from diabetes: {len(no_diabetes)}')

# let's calculate in terms of percentage
print(f'People with diabetes: {round(len(with_diabetes)/len(data)*100, 2)}%')
print(f'People who are safe from diabetes: {round(len(no_diabetes)/len(data)*100, 2)}%')

# let's check the vif 
data_preprocessed.columns

from statsmodels.stats.outliers_influence import variance_inflation_factor

variable = data_preprocessed[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']]

vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(variable.values, i) for i in range(variable.shape[1])]
vif['features'] = variable.columns

vif

# Here 'glucose','bloodpressure','BMI','age' all these factors are having values greater than 7, so, let's remove all these factores

data_preprocessed.drop(['Glucose','BloodPressure','BMI','Age'], axis=1, inplace=True)

data_preprocessed.head(10)

# let's split the data

y = data_preprocessed['Outcome']
X = data_preprocessed.drop('Outcome',axis=1)

y

X

# let's standardize the data

scalar = StandardScaler()
X_scaled = scalar.fit_transform(X)

X_scaled

# let's split the data into train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=365)

X_train.shape, X_test.shape

# let's instantiate logistic regression
from sklearn.linear_model import LogisticRegression
reg = LogisticRegression(random_state=365)
reg.fit(X_train, y_train)

reg_out = reg.predict(X_test)

reg_out

cm = confusion_matrix(y_test, reg_out)
acc = accuracy_score(y_test, reg_out)
cl = classification_report(y_test, reg_out)

print(f'''The confusion metrix is:
{cm}''')
print(f'The accuracy score is: {round(acc, 2)*100}%')
print(f'''The classification report is:
{cl}''')

# let's instantiate randomforest

forest = RandomForestClassifier(n_estimators=100, criterion='entropy',random_state=365, min_samples_split=2)
forest.fit(X_train, y_train)

forest_out = forest.predict(X_test)

forest_out

cm = confusion_matrix(y_test, forest_out)
acc = accuracy_score(y_test, forest_out)
cl = classification_report(y_test, forest_out)

print(f'''The confusion metrix is:
{cm}''')
print(f'The accuracy score is: {round(acc, 2)*100}%')
print(f'''The classification report is:
{cl}''')

# let's instantiate decision tree

tree = DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=2,random_state=365)
tree.fit(X_train, y_train)

tree_out = tree.predict(X_test)

tree_out

cm = confusion_matrix(y_test, tree_out)
acc = accuracy_score(y_test, tree_out)
cl = classification_report(y_test, tree_out)

print(f'''The confusion metrix is:
{cm}''')
print(f'The accuracy score is: {round(acc, 2)*100}%')
print(f'''The classification report is:
{cl}''')

# let's instantiate naive bayes

bayes = GaussianNB()
bayes.fit(X_train, y_train)

bayes_out = bayes.predict(X_test)

bayes_out

cm = confusion_matrix(y_test, bayes_out)
acc = accuracy_score(y_test, bayes_out)
cl = classification_report(y_test, bayes_out)

print(f'''The confusion metrix is:
{cm}''')
print(f'The accuracy score is: {round(acc, 2)*100}%')
print(f'''The classification report is:
{cl}''')

sv = SVC(kernel='rbf',random_state=365)
sv.fit(X_train, y_train)

sv_out = sv.predict(X_test)

sv_out

cm = confusion_matrix(y_test, sv_out)
acc = accuracy_score(y_test, sv_out)
cl = classification_report(y_test, sv_out)

print(f'''The confusion metrix is:
{cm}''')
print(f'The accuracy score is: {round(acc, 2)*100}%')
print(f'''The classification report is:
{cl}''')

# let's instantiate kneighbour
k = KNeighborsClassifier(n_neighbors=5, weights='uniform',algorithm='auto',metric='minkowski')
k.fit(X_train, y_train)

k_out = k.predict(X_test)

k_out

cm = confusion_matrix(y_test, k_out)
acc = accuracy_score(y_test, k_out)
cl = classification_report(y_test, k_out)

print(f'''The confusion metrix is:
{cm}''')
print(f'The accuracy score is: {round(acc, 2)*100}%')
print(f'''The classification report is:
{cl}''')

# with 69% being the highest accuracy, as there is not enough dataset to obtain it's highest accuracy

# let's go with randomforest classifier

import joblib
joblib.dump(forest, 'diabetes_prediction_model.joblib')

